{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left> \n",
    "<img src=\"../images/jds.png\" style=\"height: 100px;\" align=right> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JDS Capstone: Target Market Analysis\n",
    "\n",
    "You have been provided with information about the customers' historical purchase as the amount of money spent, Number of inactive months and so on. Two columns `Potential_Customer` and `Cust_Last_Purchase` represent the customers' respond to the latest advertisement. The column `Potential_Customer` represents if the customer purchased any product, and the column `Cust_Last_Purchase` represents the amount of this purchase and it is `Nan` if there has been no purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "- Below is the description of each feature available in the dataset.\n",
    "<img src=\"../images/MicrosoftTeams-image.png\" style=\"height: 700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "**1. Design a predictive model to determine the potential customers who will purchase if you send the advertisement .** The target variable is `Potential_Customer`. \n",
    "\n",
    "    **Attention:** Because the column `Cust_Last_Purchase` relates to the target variable `Potential_Customer`, you need to exclude it from your model.\n",
    "\n",
    "**2. Calculate the value and the revenue of your model.** Fit your model on train set. Assume amonge the customers on your test set we only send advertisement to those your model predicted as Class1 and we ignore the rest. From the data you can calculate the average `Cust_Last_Purchase` for those who are in the train set and had the last purchase (`Cust_Last_Purchase`>0) . Assume sending advertisement to each customer costs 5$ and the average purchase you calculated on the train set remains the same for the test set. Calculate the value of your models to choose the best model.\n",
    "\n",
    "    - cost = advertisement_cost * number of the predicted positive\n",
    "    - lost = average_purchase * number of the predicted negative but they have been positive\n",
    "    - gain = average_purchase * number of the predicted positive and they have been positive\n",
    "    - value = gain - lost - cost\n",
    "    - revenue = gain - cost\n",
    "\n",
    "**3. Compare your best model's revenue with the revenue of the default solution which is sending advertisement to all the customers in X_test.** Which solution would you choose?\n",
    "\n",
    "    - cost = advertisement_cost * size of the test set\n",
    "    - gain = sum(Cust_Last_Purchase) on test set\n",
    "    - revenue = gain - cost\n",
    "    \n",
    "**4. Assume the next time you want to target a group of 30,000 customers simillar to this group. And assume the purchase rate is $10%$ which means 10 out of 100 people who receive the advertisement will purchase the product. Also assume your model will have the same Precision and Recall for Class1 .** Will you send the advertisement to everyone, or you use one of the models you have already created?\n",
    "\n",
    "    - calculate your model's revenue on this set of 30,000 customers based on the above assumptions\n",
    "    - calculate the revenue of the default model: send advertisement to everyone\n",
    "         - cost = advertisement_cost * size of the test set\n",
    "         - gain = average_purchase * purchase_rate\n",
    "         - revenue = gain - cost\n",
    "    \n",
    "**Hint:**\n",
    "    To calculate the revenue of a model for this new set of customers with different purchase rate we need to calculate the new confusion matrix given Precision and Recall for Class1 are fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guideline\n",
    "\n",
    "Perform the following:\n",
    "\n",
    "1. Create a team (2-3 people)\n",
    "2. Perform the following:\n",
    "    1. **Data Wrangling - Cleaning & Merging**: Check and handle the existance of missing values, the type of variables, or integrity of data\n",
    "    2. **Exploratory Data Analysis**: Analyze data to summarize their main characteristics\n",
    "    3. **Feature Engineering**: Make new features or change the current features\n",
    "    4. **Feature Selection**: Choose the best features\n",
    "    5. **Data Pre-Processing**: Make data usable for applying ML algorithms. \n",
    "    6. **Model Design**: Create several predictive models and tune the hyperparameters\n",
    "    7. **Model Evaluation**: Compare the performance of the models\n",
    "    8. **Bonus**: Any creative idea for improving machine learning models\n",
    "\n",
    "The output expected at the end of this capstone is:\n",
    "1. One **Jupyter notebook** containing all analysis performed using Python.\n",
    "2. One **PowerPoint** presentation containing the analysis results. Each group will be allocated 15-20 minutes (inclusive of Q&A) to present their analysis results to the class. \n",
    "\n",
    "**One zip file per group** is to be uploaded on **GDrive** by the **time** that will be given to you by the trainer, including the **jupyter notebook(s)** and the **powerpoint presentation** indicating the **names of all group members**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation Guideline\n",
    "\n",
    "**Note(s):**\n",
    "1. Only **one submission** is required per group.\n",
    "2. Include full details in your notebook and **report only important results** in your presentation.\n",
    "3. Please only use Jupyter notebook (pandas) to clean the data (Do not clean manually using Excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "- [JDS Capstone: Target Market Analysis](#JDS-Capstone:-Target-Market-Analysis)\n",
    "- [Data description](#Data-description)\n",
    "- [Objective](#Objective)\n",
    "- [Submission Guideline](#Submission-Guideline)\n",
    "- [Presentation Guideline](#Presentation-Guideline)\n",
    "- [Table of Content](#Table-of-Content)\n",
    "- [0. Import necessary Packages](#0.-Import-necessary-Packages)\n",
    "- [1. Load the Data into Pandas Dataframe](#1.-Load-the-Data-into-Pandas-Dataframe)\n",
    "- [2. Data Cleaning](#2.-Data-Cleaning)\n",
    "    - [2.1 How big is the dataset? (number of rows, features and total datapoints)](#2.1-How-big-is-the-dataset?-(number-of-rows,-features-and-total-datapoints))\n",
    "    - [2.2 What is the type of each column?](#2.2-What-is-the-type-of-each-column?)\n",
    "        - [2.2.1 Why columns such as `Cust_Last_Purchase` are `object` while they should be `float64`? Fix the type of the columns as it should be.](#2.2.1-Why-columns-such-as-`Cust_Last_Purchase`-are-`object`-while-they-should-be-`float64`?-Fix-the-type-of-the-columns-as-it-should-be.)\n",
    "    - [2.3 Check data for duplicate rows and remove the duplicates](#2.3-Check-data-for-duplicate-rows-and-remove-the-duplicates)\n",
    "    - [2.4 Do we need `C_ID` in our analysis? Drop the columns you will not use in your analysis, if there is any.](#2.4-Do-we-need-`C_ID`-in-our-analysis?-Drop-the-columns-you-will-not-use-in-your-analysis,-if-there-is-any.)\n",
    "- [3. Exploratory Data Analysis (EDA)](#3.-Exploratory-Data-Analysis-(EDA))\n",
    "    - [3.1 Explore Categorical Variables](#3.2-Explore-Categorical-Variables)\n",
    "        - [3.1.1 Insight](#3.1.1-Insight)\n",
    "        - [3.1.2 Solution](#3.1.2-Solution)\n",
    "    - [3.2 Explore Relationship Between Categorical & Target Variable. Interpret the observation](#3.2-Explore-Relationship-Between-Categorical-&-Target-Variable.-Interpret-the-observation)\n",
    "        - [3.2.1. Insight](#3.2.1.-Insight)\n",
    "    - [3.3 Explore Numerical Variables](#3.3-Explore-Numerical-Variables)\n",
    "        - [3.3.1 Insight](#3.3.1-Insight)\n",
    "    - [3.4 Explore the Relationship between Numerical Variables & Target Variable. Interpret your observation](#3.4-Explore-the-Relationship-between-Numerical-Variables-&-Target-Variable.-Interpret-your-observation)\n",
    "    - [3.5 Explore the Relationship between the columns and try to answer the following questions:](#3.5-Explore-the-Relationship-between-the-columns-and-try-to-answer-the-following-questions:)\n",
    "- [4. Feature Enginearing](#4.-Feature-Enginearing)\n",
    "    - [4.1 Add Some High Level Features and explore their relationship with the target variable](#4.1-Add-Some-High-Level-Features-and-explore-their-relationship-with-the-target-variable)\n",
    "    - [4.2 Check Correlation between Numerical Variables](#4.2-Check-Correlation-between-Numerical-Variables)\n",
    "- [5. Feature Selection](#5.-Feature-Selection)\n",
    "- [6. Data PreProcessing](#6.-Data-PreProcessing)\n",
    "    - [6.1 Check the Data for Missing Values](#6.1-Check-the-Data-for-Missing-Values)\n",
    "    - [6.2 Separate X (features) and y (target)](#6.2-Separate-X-(features)-and-y-(target))\n",
    "    - [6.3 Split data to train/test](#6.3-Split-data-to-train/test)\n",
    "    - [6.4 Dummy Variables](#6.4-Dummy-Variables)\n",
    "    - [6.5 Feature Scaling](#6.5-Feature-Scaling)\n",
    "    - [6.6 PCA on Numerical Columns only](#6.6-PCA-on-Numerical-Columns-only)\n",
    "- [7. Objective 1: Machine Learning](#7.-Objective-1:-Machine-Learning)\n",
    "- [8. Objective 2](#8.-Objective-2)\n",
    "- [9. Objective 3](#9.-Objective-3)\n",
    "- [10. Objective 4](#10.-Objective-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   0. Import necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics.scorer import SCORERS, roc_auc_score, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. Load the Data into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into data dataframe\n",
    "data = pd.read_csv('../data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first 5 rows of dataframe\n",
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View your data\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning\n",
    "Checking the existance of missing values, the type of variables, or integrity of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 How big is the dataset? (number of rows, features and total datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 What is the type of each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Why columns such as `Cust_Last_Purchase` are `object` while they should be `float64`? Fix the type of the columns as it should be.\n",
    "\n",
    "**Attention:** Some numerical columns have missing values, Dollar sign, or Comma. You need to fix the issue to be able to convert the column to numerical. \n",
    "\n",
    "**Hint:** \n",
    "1. The following code can help you to remove an 'OldSign' and replace it with a 'NewSign' or nothing: `df.col=df.col.str.replace('OldSign', 'NewSign')`\n",
    "\n",
    "2. After removing the signs and replace it with correct sign, or nothing you need to:\n",
    "\n",
    "    a- Create a list of the name of the categorical columns and the numerical columns:\n",
    "        `CatCols=[Name of the Categorical columns]`\n",
    "        `NumCols=list(set(data.columns)-set(CatCols))`\n",
    "    \n",
    "    b- Fix the type of the columns\n",
    "        `data[CatCols] = data[CatCols].apply(lambda x: x.astype('category'))`\n",
    "        `data[NumCols] = data[NumCols].apply(lambda x: x.astype('float64'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Check data for duplicate rows and remove the duplicates\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. `data.duplicated()` will give you `True` if the row in `data` is duplicate and `False` otherwise.\n",
    "\n",
    "2. `duplicates.sum()` will tell you how many duplicates you have in `data`.\n",
    "\n",
    "3. `data=data.drop_duplicates()` will remove the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Do we need `C_ID` in our analysis? Drop the columns you will not use in your analysis, if there is any.\n",
    "\n",
    "**Hint:** \n",
    "1. Drop the useless column(s)\n",
    "2. Remove the name of the column(s) from `CatCols` or `NumCols`\n",
    "    Example: CatCols.remove('C_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)\n",
    "    \n",
    "Checking the relationship betweem variables, summary of data, outliers, filling missing values etc.\n",
    "**If the ultimate goal is designing predictive models on the data, and we use EDA as part of the proprocessing, we are NOT allowed to do EDA on the test set.** However, if you only do EDA to get business insight from the data, you **CAN** use the whole data, if you don't use that insight for data preprocessing such as data cleaning.\n",
    "    \n",
    "**Example:** To impute the missing values by mean/median, we calculate the mean or the median on the Train set only and then we impute the missing values by that mean/median on both Train and Test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Explore Categorical Variables\n",
    "1. How many categories in each categorical variables?\n",
    "2. What proportion/percentage from each category?\n",
    "\n",
    "**Hint:**\n",
    "For visualization you can use `sns.countplot()` for each categorical variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Explore Relationship Between Categorical & Target Variable. Interpret the observation\n",
    "\n",
    "**Hint**: \n",
    "1. Create list of the categorical features:\n",
    "    `CatFes=list(set(CatCols)-set(['Potential_Customer']))`\n",
    "\n",
    "2. use `sns.countplot()` to create subplots for each categorical feature and hue=`data.Potential_Customer` to assign color to the plot based on the target variable `Potential_Customer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Explore Numerical Variables\n",
    "\n",
    "**Hint**: use `sns.distplot()` and `sns.boxplot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Explore the Relationship between Numerical Variables & Target Variable. Interpret your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Explore the Relationship between the columns and try to answer the following questions:\n",
    "\n",
    "1. Is there any significant difference between men/women's salary?\n",
    "\n",
    "2. Is there any significant difference between men/women's number of the purchase in the last three years?\n",
    "\n",
    "3. Is there any significant difference between men/women's average purchase in the last three years?\n",
    "\n",
    "4. Is there any significant difference between men/women's total purchase in the last three years?\n",
    "\n",
    "5. (optional) You can explore more about the relationships between the columns, if you believe the insight will improve some dicisions in this company. For instance, sending advertisements to customers regarding gender, customer status, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Enginearing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Add Some High Level Features and explore their relationship with the target variable\n",
    "\n",
    "Sometimes we can use high level features that reflect the interactions between the columns as new features to get better insight and feed more information to our predictive models. Also transformation of some columns can be better options to be fed to the models. For instance, instead of a numerical column, you can use log of the column, square of the column, or any other transformation of the column. The type of interaction, or transformation you should choose, can be defined after Exploratory data analysis or just business insight.\n",
    "\n",
    "**Example:** Instead of two variables for the number of purchase, and the average amount of purchase, we could calculate the total amount of purchase for the customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Check Correlation between Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Selection\n",
    "\n",
    "It is better we do not have numerical columns with high correlations as they confuse the machine learning algorithms. We can manually remove the highly-correlated features, or we can let the PCA handles that during the pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Check the Data for Missing Values\n",
    "**Hint:**\n",
    "1. Check which columns have missing values\n",
    "\n",
    "2. Create a list of the name of the columns that have missing values\n",
    "    `null_columns=data.columns[data.isnull().any()]`\n",
    "   \n",
    "3. Decide how you should handle the missing values for each column:\n",
    "\n",
    "    a. For some numerical columns missing value simply means 0.\n",
    "    b. We can fill missing values in a numerical column by replacing mean of the column, if the column is not skewed. If the column is skewed, median might be a better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Separate X (features) and y (target)\n",
    "\n",
    "**Attention:** Don't forget to exclude the column `Cust_Last_Purchase` from your analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Split data to train/test \n",
    "\n",
    "Define X and y and split the data into 75/25 train/test set. Use random_state=42 and stratify=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Dummy Variables\n",
    "Change categorical variables with numerical variabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 PCA on Numerical Columns only\n",
    "\n",
    "1. Save the above scaled train and test data, as dataframe with proper column names\n",
    "`X_train_sc=pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)`\n",
    "2. Separate train and test data for numerical columns only\n",
    "`train_PCA=X_train_sc[NewNumCols]`, `test_PCA=X_test_sc[NewNumCols]`\n",
    "3. Define the number of components on `train_PCA`\n",
    "4. Fit PCA on `train_PCA` \n",
    "5. Transform `train_PCA` and `test_PCA` save it as `PCA_train` and `PCA_test`, and save them as DataFrame. Use `PCA_train.index=X_train.index` to make sure `PCA_train` have the same index with `X_train` because we need to concat this data to the dummy variables. Do the same on `PCA_test`\n",
    "6. Concat `PCA_train` to the dummy variables in `X_train` save it as `X_train_pca`\n",
    "7. Concat `PCA_test` to the dummy variables in `X_test` save it as `X_train_pca`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Objective 1: Machine Learning\n",
    "    \n",
    "**1. Design a predictive model to determine the potential customers who will purchase if you send the advertisement .** The target variable is `Potential_Customer`. \n",
    "\n",
    "    **Attention:** Because the column `Cust_Last_Purchase` relates to the target variable `Potential_Customer`, you need to exclude it from your model.\n",
    "\n",
    "\n",
    "Apply various ML algorithms on the data, evaluate them after Grid Search and Cross Validation, and choose the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Objective 2\n",
    "\n",
    "**2. Calculate the value and the revenue of your model.** Fit your model on train set. Assume amonge the customers on your test set we only send advertisement to those your model predicted as Class1 and we ignore the rest. From the data you can calculate the average `Cust_Last_Purchase` for those who are in the train set and had the last purchase (`Cust_Last_Purchase`>0) . Assume sending advertisement to each customer costs 5$ and the average purchase you calculated on the train set remains the same for the test set. Calculate the value of your models to choose the best model.\n",
    "\n",
    "    - cost = advertisement_cost * number of the predicted positive\n",
    "    - lost = average_purchase * number of the predicted negative but they have been positive\n",
    "    - gain = average_purchase * number of the predicted positive and they have been positive\n",
    "    - value = gain - lost - cost\n",
    "    - revenue = gain - cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Objective 3\n",
    "    \n",
    "**3. Compare your best models' revenue with the revenue of the default solution which is sending advertisement to all the customers in X_test.** Which solution would you choose?\n",
    "\n",
    "    - cost = advertisement_cost * size of the test set\n",
    "    - gain = sum(Cust_Last_Purchase) on test set\n",
    "    - revenue = gain - cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Objective 4\n",
    "    \n",
    "**4. Assume the next time you want to target a group of 30,000 customers simillar to this group. And assume the purchase rate is $10%$ which means 10 out of 100 people who receive the advertisement will purchase the product. Also assume your model will have the same Precision and Recall for Class1 .** Will you send the advertisement to everyone, or you use one of the models you have already created?\n",
    "\n",
    "    - calculate your model's revenue on this set of 30,000 customers based on the above assumptions\n",
    "    - calculate the revenue of the default model: send advertisement to everyone\n",
    "         - cost = advertisement_cost * size of the test set\n",
    "         - gain = average_purchase * purchase_rate\n",
    "         - revenue = gain - cost\n",
    "    \n",
    "**Hint:**\n",
    "    To calculate the revenue of a model for this new set of customers with different purchase rate we need to calculate the new confusion matrix given Precision and Recall for Class1 are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
